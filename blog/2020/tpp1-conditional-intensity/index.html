<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Temporal Point Processes 1: The Conditional Intensity Function | Oleksandr Shchur </title> <meta name="author" content="Oleksandr Shchur"> <meta name="description" content="How can we define generative models for variable-length event sequences in continuous time?"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://shchur.github.io/blog/2020/tpp1-conditional-intensity/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Temporal Point Processes 1: The Conditional Intensity Function",
            "description": "How can we define generative models for variable-length event sequences in continuous time?",
            "published": "December 17, 2020",
            "authors": [
              
              {
                "author": "Oleksandr Shchur",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Oleksandr</span> Shchur </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/publications/">publications</a> <div class="dropdown-divider"></div> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/blog/">blog</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Temporal Point Processes 1: The Conditional Intensity Function</h1> <p>How can we define generative models for variable-length event sequences in continuous time?</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div><a href="#what-is-a-point-process"> What is a point process?</a></div> <div><a href="#tpp-as-an-autoregressive-model">TPP as an autoregressive model</a></div> <div><a href="#defining-tpps-using-the-conditional-intensity-function">Defining TPPs using the conditional intensity function</a></div> <div><a href="#tpp-as-a-counting-process">TPP as a counting process</a></div> <div><a href="#summary">Summary</a></div> </nav> </d-contents> <h2 id="tldr">TL;DR</h2> <ul> <li>Temporal point processes (TPPs) are probability distributions over variable-length event sequences in continuous time.</li> <li>We can view a TPP as an autoregressive model or as a counting process.</li> <li>The conditional intensity function $\lambda^*(t)$ connects these two viewpoints and allows us to specify TPPs with different behaviors, such as a global trend or burstiness.</li> <li>The conditional intensity $\lambda^*(t)$ is one of many ways to define a TPP — as an alternative, we could, for example, specify the conditional PDFs of the arrival times \(\{f_1^*, f_2^*, f_3^*, ...\}\).</li> </ul> <h2 id="what-is-a-point-process">What is a point process?</h2> <p>Probabilistic generative models are the bread and butter of modern machine learning. They allow us to make predictions, find anomalies and learn useful representations of the data. Most of the time, applying the generative model involves learning the probability distribution \(P(\boldsymbol{x})\) over our data points \(\boldsymbol{x}\).</p> <p>We know what to do if \(\boldsymbol{x}\) is a vector in \(\mathbb{R}^D\) — simply use a multivariate Gaussian or, if we need something more flexible, our favorite <a href="https://arxiv.org/abs/1912.02762" rel="external nofollow noopener" target="_blank">normalizing flow</a> model. But what if a single realization of our probabilistic model corresponds to a <em>set</em> of vectors \(\{\boldsymbol{x}_1, ..., \boldsymbol{x}_N\}\)? Even worse, what if both \(N\), the number of the vectors, as well as their locations \(\boldsymbol{x}_i\) are random? This is not some hypothetical scenario — processes generating such data are abundant in the real world:</p> <ul style="margin-top: 0px"> <li> Transactions generated each day in a financial system</li> <li> Locations of disease outbreaks in a city, recorded each week</li> <li> Times and locations of earthquakes in some geographic region within a year</li> </ul> <p>Point processes provide a framework for modeling and analyzing such data. Each realization of a point process is a set \(\{\boldsymbol{x}_1, \dots, \boldsymbol{x}_N\}\) consisting of a random number \(N\) of <em>points</em> \(\boldsymbol{x}_i\) that live in some space \(\mathcal{X}\), hence the name “point process”. Depending on the choice of the space \(\mathcal{X}\), we distinguish among different types of point processes. For example, \(\mathcal{X} \subseteq \mathbb{R}^D\) corresponds to a so-called <em>spatial point process</em>, where every point \(\boldsymbol{x}_i\) can be viewed as a random location in space (e.g., a location of a disease outbreak).</p> <div class="l-body"> <img class="img-fluid rounded" src="/assets/img/posts/tpp1/spp_sample.png" style="display: block; width: 90%; margin-left: auto; margin-right: auto;"> <figcaption style="text-align: center; margin-top: 10px; margin-bottom: 10px;"> Two realizations of a spatial point process on $\mathbb{R}^2$. </figcaption> </div> <p>Another important case, to which I will dedicate the rest of this post (and, hopefully, several future ones), are <em>temporal point processes</em> (TPPs), defined on the real half-line \(\mathcal{X} \subseteq [0, \infty)\). We can interpret the points in a TPP as events happening in continuous time, and therefore usually denote them as \(t_i\) (instead of \(\boldsymbol{x}_i\)).</p> <div class="l-body"> <img class="img-fluid rounded" src="/assets/img/posts/tpp1/tpp_sample.png" style="margin-left: auto; margin-right: auto;"> <figcaption style="text-align: center; margin-top: 10px; margin-bottom: 10px;"> Two realizations of a temporal point process on $[0, T]$. </figcaption> </div> <p>At first it might seem like TPPs are just a (boring) special case of spatial point processes, but this is not true. Because of the ordered structure of the set \([0, \infty)\), we can treat TPP realizations (i.e., sets \(\{t_1, \dots, t_N\}\)) as ordered sequences \(\boldsymbol{t} = (t_1, \dots, t_N)\), where \(t_1 &lt; t_2 &lt; \dots &lt; t_N\). Additionally, we typically assume that the arrival time of the event \(t_i\) is only influenced by the events that happened in the past. As we will see in the next section, this makes specifying TPP distributions rather easy. In contrast, spatial point processes don’t permit such ordering on the events, and because of this often have intractable densities.</p> <p>The theory of temporal point processes was mostly developed near the middle of the 20th century, taking roots in measure theory and stochastic processes. For this reason, the notation and jargon used in TPP literature may sound strange and unfamiliar to people with a machine learning background (at least it did to me back when I started learning about TPPs). In reality, though, most TPP-related concepts can be easily translated into the familiar language of probabilistic machine learning.</p> <p>In this post we will investigate different ways to represent a TPP. As we will see, a TPP can be treated as an autoregressive model or as a counting process. We will learn about the conditional intensity function \(\lambda^*(t)\) — a central concept in the theory of point processes — that unites these two perspectives and allows us to compactly describe various TPP distributions.</p> <h2 id="tpp-as-an-autoregressive-model">TPP as an autoregressive model</h2> <p>How do we define a probabilistic model that generates variable-length event sequences \(\boldsymbol{t} = (t_1, \dots, t_N)\)<d-footnote>Some technicalities: We usually assume that our TPPs are <i>simple</i>. This means that (1) the number of events $N$ is finite almost surely (=with probability one) and (2) the arrival times $t_i$ are distinct, i.e. $t_i \ne t_j$ for all $i\ne j$. Additionally, we assume that $t_i$'s are continuous random variables. This means, among other things, that $\Pr(t_i \in [a, b]) = \Pr(t_i \in (a, b))$, i.e., we shouldn't worry about the interval boundaries too much.</d-footnote> in the interval \([0, T]\)? Thanks to the inherent ordering on the events, we could define our model autoregressively. We start by sampling \(t_1\), the time of the first event, from some probability distribution \(P_1(t_1)\) that is supported on \([0, \infty)\). If \(t_1 &gt; T\), i.e., the event happened outside of the observed interval, we are done — our realization \(\boldsymbol{t}\) is just an empty sequence. Otherwise, we sample the next event \(t_2\) from the conditional distribution \(P_2(t_2 | t_1)\) that is supported on \([t_1, \infty)\). Again, we check if \(t_2 &gt; T\), and if not, proceed to sample \(t_3\) from \(P_3(t_3 | t_1, t_2)\). We repeat this process until some event \(t_{N+1}\) falls outside of the observed interval, at which point we stop the process and get our sample consisting of \(N\) events.</p> <p>At each step we are dealing with the conditional distribution of the event \(t_i\) given the <em>history</em> of the past events \(\mathcal{H}_{t_i} = \{t_j: t_j &lt; t_i\}\). We usually denote this distribution as \(P_i(t_i | \mathcal{H}_{t_i})\). In the literature, you will also often meet the shorthand notation \(P_i^*(t_i)\), where the star reminds us of the dependency on past events. The important question is how to represent the probability distribution \(P_i^*(t_i)\).</p> <p>In machine learning, we usually characterize a continuous probability distribution \(P_i^*\) by specifying its probability density functions (PDF) \(f_i^*\). Loosely speaking, the value \(f_i^*(t) dt\) represents the probability that the event \(t_i\) will happen in the interval \([t, t + dt)\), where \(dt\) is some infinitesimal positive number.<d-footnote>All the explanations involving $dt$ are <a href="https://en.wikipedia.org/wiki/Differential_(infinitesimal)" rel="external nofollow noopener" target="_blank">not 100% rigorous</a> and are used to provide intuition — if we set $dt$ to some tiny positive number, then the equations would be approximately correct. Turning such a handwavy explanation into a rigorous mathematical argument would require taking the limit $dt \to 0$.</d-footnote></p> <p>However, there exist other ways to describe a distribution that might be more useful in certain contexts. For example, the <em>cumulative distribution function</em> (CDF) \(F_i^*(t) = \int_0^{t} f_i^*(u) du\) tells us the probability that the event \(t_i\) will happen before time \(t\). Closely related is the <em>survival function</em> (SF), defined as \(S_i^*(t) = 1 - F_i^*(t)\), which tells us the probability that the event \(t_i\) will happen <em>after</em> time \(t\).</p> <div class="l-body"> <img class="img-fluid rounded" src="/assets/img/posts/tpp1/pdf_cdf_sf.png" style="display: block; width: max(80%, 200px); margin-left: auto; margin-right: auto;"> <figcaption style="text-align: center; margin-top: 10px; margin-bottom: 10px;"> Interpretation of the PDF, CDF and SF. Here $\mathcal{H}_t = \{t_1, ..., t_{i-1}\}$. </figcaption> </div> <p>Finally, a lesser known option is the <a href="https://en.wikipedia.org/wiki/Failure_rate" rel="external nofollow noopener" target="_blank"><em>hazard function</em></a> \(h_i^*\) that can be computed as \(h^*_i(t) = f_i^*(t) / S_i^*(t)\). The value \(h_i^*(t)dt\) answers the question “What is the probability that the event \(t_i\) will happen in the interval \([t, t + dt)\) given that it didn’t happen before \(t\)?”. Let’s look at this definition more closely to examine the connection between the PDF \(f_i^*\) and the hazard function \(h_i^*\).</p> <p>Consider the following scenario. The most recent event \(t_{i-1}\) has just happened and our clock is at time \(t_{i-1}\). The value \(f_i^*(t)dt\) tells us the probability that the next event \(t_i\) will happen in \([t, t+ dt)\) (see next figure — top). Then, some time has elapsed, our clock is now at time \(t\) and the event \(t_{i}\) hasn’t yet happened. At this point in time, \(f_i^*(t)dt\) is not equal to \(\Pr(t_i \in [t, t + dt) | \mathcal{H}_t)\) anymore — we need to condition on the fact that \(t_i\) didn’t happen before \(t\). For this, we renormalize the PDF such that it integrates to \(1\) over the interval \([t, \infty)\) (see next figure — center).</p> \[f_i^*(t | t_i \ge t) = \frac{f_i^*(t)}{\int_t^\infty f_i^*(u) du} =\frac{f_i^*(t)}{S_i^*(t)} =: h_i^*(t)\] <p>This value of the renormalized PDF exactly corresponds to the hazard function \(h_i^*\) at time \(t\) (see next figure — bottom).</p> <div class="l-body"> <img class="img-fluid rounded" src="/assets/img/posts/tpp1/renorm_pdf.png" style="display: block; width: max(80%, 200px); margin-left: auto; margin-right: auto;"> <figcaption style="text-align: center; margin-top: 10px; margin-bottom: 10px;"> PDF of $t_i$ (top), PDF of $t_i$ conditioned on $t_i \ge t$ (center), hazard function of $t_i$ (bottom). </figcaption> </div> <p>We can also go in the other direction and compute the PDF \(f_i^*\) using \(h_i^*\). First, we need to compute the survival function</p> \[\begin{aligned} h_i^*(t) &amp;= \frac{f_i^*(t)}{S_i^*(t)} = \frac{- \frac{d}{dt} S_i^*(t)}{S_i^*(t)} = -\frac{d}{dt} \log S_i^*(t)\\ &amp; \Leftrightarrow S_i^*(t) = \exp \left( -\int_{t_{i-1}}^t h_i^*(u) du \right) \end{aligned}\] <p>This, in turn, allows us to obtain the PDF as</p> \[\begin{aligned} f_i^*(t) &amp;= -\frac{d}{dt}S_i^*(t)\\ &amp; = -\frac{d}{dt}\exp \left( -\int_{t_{i-1}}^t h_i^*(u) du \right)\\ &amp;= h_i^*(t) \exp \left( -\int_{t_{i-1}}^t h_i^*(u) du \right) \end{aligned}\] <p>The name “hazard function” comes from the field of <a href="https://en.wikipedia.org/wiki/Survival_analysis" rel="external nofollow noopener" target="_blank">survival analysis</a>, where the goal is to predict hazardous events such as death of a patient or failure of some system. In such a setting, the hazard function \(h_i^*\) is often considered to be more interpretable<d-footnote>In my opinion, only very basic hazard functions are somewhat interpretable --- for example, if the hazard function monotonically increases as $t \to \infty$ (e.g., older people are more likely to die at any given time) or decreases (e.g., if a light bulb didn't break in the first hour of operation, then it's not defect and will serve for a long time). However, if your hazard function is defined by a neural network (e.g., a normalizing flow), I would argue that it's as (un)interpretable as the PDF $f_i^*.$</d-footnote> than the PDF $f_i^*$. For example, if a system hasn’t failed by time \(t_i\), the value \(h_i^*(t)dt\) corresponds to the probability of failure in the immediate future. This quantity can be of interest when planning treatments or allocating resources.</p> <p>Let’s get back to our problem of characterizing the conditional distributions of a TPP. We could specify any of the functions \(f_i^*\), \(F_i^*\), \(S_i^*\) or \(h_i^*\) (subject to the respective constraints<d-footnote>Constraints on PDF, CDF, SF and hazard function are necessary to ensure that they define a valid probability distribution. For example, a PDF $f_i^*$ must satisfy $f_i^*(t) \ge 0$ for all $t$ and $\int_{t_{i-1}}^\infty f_i^*(u) du = 1$. Similarly, a valid hazard function $h_i^*$ must satisfy $h_i^*(t) \ge 0$ for all $t$ and $\int_{t}^{\infty} h_i^*(u) du = \infty$ for all $t$.</d-footnote>), and each one of them would completely describe the distribution \(P_i^*\). Put differently, given one of these functions, we can directly compute the other three. It’s worth noting that there exist other functions (besides the four mentioned above) that we could use to describe a distribution.</p> <div class="l-body"> <img class="img-fluid rounded" src="/assets/img/posts/tpp1/pdf_cdf_sf_hazard.png"> <figcaption style="text-align: center; margin-top: 10px; margin-bottom: 10px;"> Four ways to represent the conditional distribution $P_i^*(t)$: probability density function $f_i^*$, cumulative distribution function $F_i^*$, survival function $S_i^*$, and hazard function $h_i^*$. </figcaption> </div> <p>In summary, to define the full distribution of some TPP, we could, for instance, specify the conditional PDFs \(\{f_1^*, f_2^*, f_3^*, \dots\}\) or, equivalently, the conditional hazard functions \(\{h_1^*, h_2^*, h_3^*, \dots\}\). However, dealing with all the different conditional distributions and their indices can be unwieldy. Instead, we could consider yet another way of characterizing the TPP — using the <em>conditional intensity function</em>. The conditional intensity, denoted as \(\lambda^*(t)\), is defined by stitching together the conditional hazard functions:</p> \[\lambda^*(t) = \begin{cases} h_1^*(t) &amp; \text{ if } 0 \le t \le t_1 \\ h_2^*(t) &amp; \text{ if } t_1 &lt; t \le t_2 \\ &amp; \vdots\\ h_{N+1}^*(t) &amp; \text{ if } t_N &lt; t \le T \\ \end{cases}\] <p>which can graphically be represented as follows:</p> <div class="l-body"> <img class="img-fluid rounded" src="/assets/img/posts/tpp1/stitching.png" style="display: block; width: max(80%, 200px); margin-left: auto; margin-right: auto;"> <figcaption style="text-align: center; margin-top: 10px; margin-bottom: 10px;"> The conditional intensity $\lambda^*(t)$ is obtained by stitching together the hazard functions $h_i^*(t)$. </figcaption> </div> <p>Let’s take a step back and remember what the \(*\) notation means here. When we write \(\lambda^*(t)\), we actually mean \(\lambda(t | \mathcal{H}_t)\). That is, the conditional intensity function takes as input two arguments: (1) the current time \(t\) and (2) the set of the preceding events \(\mathcal{H}_t\) that can be of arbitrary size. </p> <p>We can turn the previous statement around: To define a TPP distribution, we simply need to define some non-negative function<d-footnote>The conditional intensity must be non-negative since it's a ratio of two non-negative numbers — the PDF $f_i^*(t)$ and the SF $S_i^*(t)$. There is another technical detail: it should hold for any $t$ and $\mathcal{H}_t$ that $\int_t^{\infty} \lambda(u \vert \mathcal{H}_t) du = \infty$. If the latter condition is not fulfilled, the respective conditional PDF $f_i^*$ won't integrate to 1.</d-footnote> that takes as input the time \(t \in [0, T]\) and a variable-sized set of past events \(\{t_1, \dots, t_{i-1}\}\). This will completely specify the conditional intensity \(\lambda^*(t)\). Given \(\lambda^*(t)\), we can easily recover the conditional hazard functions \(h_i^*\). Finally, we can obtain the conditional PDFs \(f_i^*\) from the \(h_i^*\)’s. Thus, we have completely specified our TPP distribution. Neat!</p> <h2 id="defining-tpps-using-the-conditional-intensity-function">Defining TPPs using the conditional intensity function</h2> <p>The main advantage of the conditional intensity is that it allows to compactly represent various TPPs with different behaviors. For example, we could define a TPP where the intensity is independent of the history and only depends on the time \(t\).</p> \[\lambda^*(t) = g(t)\] <p>This corresponds to the famous <a href="https://en.wikipedia.org/wiki/Poisson_point_process" rel="external nofollow noopener" target="_blank">Poisson process</a>. High values of \(g(t)\) correspond to a higher rate of event occurrence, so the Poisson process allows us to capture global trends. For instance, we could use it to model passenger traffic in a subway network within a day. More events (i.e., ticket purchases) happen in the morning and in the evening compared to the middle of the day, which is reflected by the variations in the intensity \(g(t)\).</p> <div class="l-body"> <img class="img-fluid rounded" src="/assets/img/posts/tpp1/poisson.png" style="display: block; width: max(80%, 200px); margin-left: auto; margin-right: auto;"> <figcaption style="text-align: center; margin-top: 10px; margin-bottom: 10px;"> A realization of a Poisson process (bottom) and the respective intensity function (top). </figcaption> </div> <p>The Poisson process has a number of other interesting properties and probably deserves a blog post of its own.</p> <p>Another popular example is the <a href="https://arxiv.org/abs/1708.06401" rel="external nofollow noopener" target="_blank">self-exciting process (a.k.a. Hawkes process)</a> with the conditional intensity function</p> \[\lambda^*(t) = \mu + \sum_{t_j \in \mathcal{H}_t} \alpha \exp(-(t - t_j))\] <p>As we see above, the intensity increases by \(\alpha\) whenever an event occurs and then exponentially decays towards the baseline level \(\mu\). Such an intensity function allows us to capture “bursty” event occurrences — events often happen in quick succession. For example, if a neuron fires in the brain, it’s likely that this neuron will fire again in the near future.</p> <div class="l-body"> <img class="img-fluid rounded" src="/assets/img/posts/tpp1/hawkes.png" style="display: block; width: max(80%, 200px); margin-left: auto; margin-right: auto;"> <figcaption style="text-align: center; margin-top: 10px; margin-bottom: 10px;"> A realization of a Hawkes process (bottom) and the respective intensity function (top). </figcaption> </div> <p>Both of the above examples could equivalently be specified using the conditional PDFs \(f_i^*\) or the hazard functions \(h_i^*\). However, their description in terms of the conditional intensity \(\lambda^*(t)\) is more elegant and compact — we don’t have to worry about the indices \(i\), and we can understand the properties of respective TPPs (such as global trend or burstiness) by simply looking at the definition of \(\lambda^*(t)\).</p> <h2 id="tpp-as-a-counting-process">TPP as a counting process</h2> <p>So far, we have represented TPP realizations as variable-length sequences, but this is not the only possible option. In many textbooks and (especially older) papers a TPP is defined as a <em>counting process</em> <d-footnote>A counting process is a special type of stochastic processes. A <a href="https://en.wikipedia.org/wiki/Stochastic_process" rel="external nofollow noopener" target="_blank">stochastic process</a> is a collection of random variables that are indexed by a real number. In our case, any real number $t \in [0, T]$ corresponds to a random variable $N(t).$</d-footnote> — a probability distribution over functions. Each realization of a counting process is an increasing function \(N \colon [0, T] \to \mathbb{N}_0\). We can think of \(N(t)\) as the number of events that happened before time \(t \in [0, T]\).</p> <p>It’s easy to see that this formulation is equivalent to the one we used before. We can represent an event sequence \(\boldsymbol{t} = (t_1, \dots, t_N)\) as a realization of a counting process by defining</p> \[N(t) = \sum_{i=1}^N \mathbb{I}(t_i \le t)\] <p>where \(\mathbb{I}\) is the indicator function.</p> <div class="l-body"> <img class="img-fluid rounded" src="/assets/img/posts/tpp1/counting.png" style="display: block; width: max(80%, 200px); margin-left: auto; margin-right: auto;"> <figcaption style="text-align: center; margin-top: 10px; margin-bottom: 10px;"> Realization of a counting process (above) and the respective event sequence (below). </figcaption> </div> <p>Last, we will consider is how to characterize the distribution of a counting process. Not surprisingly, the conditional intensity function \(\lambda^*(t)\) that we defined in the previous section will again come up here.</p> <p>Like before, suppose that \(dt\) is an infinitesimal positive number. We will consider the expected change in \(N(t)\) during \(dt\) given the history of past events \(\mathcal{H}_t\), that is<d-footnote>The first equality follows from the assumption that $dt$ is small enough that at most a single event can happen in the interval $[t, t+ dt)$. Therefore, $N(t + dt) - N(t)$ can take only two values: either 1 or 0. The outcome "$N(t + dt) - N(t) = 1$" can be rephrased as "the event $t_i$ happened in the interval $[t, t+ dt)$ given that it didn't happen in the interval $[t_{i-1}, t)$" (where $t_{i-1}$, without loss of generality, is the last event that happened before time $t$). The probability of this can be computed using the conditional hazard function $h_i^*(t)$, which by definition is equal to the conditional intensity $\lambda^*(t)$.</d-footnote></p> \[\begin{aligned} \mathbb{E}[N(t + dt) - N(t) | \mathcal{H}_t] =&amp; \;1 \cdot \Pr(\text{next event } t_i \text{ happens in } [t, t + dt) | \mathcal{H}_t)\\ &amp;+ 0 \cdot \Pr(\text{no event in } [t, t + dt) | \mathcal{H}_t)\\ =&amp; \Pr(t_i \in [t, t+ dt) | \mathcal{H}_{t})\\ =&amp; h_i^*(t) dt\\ =&amp; \lambda^*(t) dt\\ \end{aligned}\] <p>By rearranging the above equation we could define the conditional intensity function as</p> \[\lambda^*(t) = \lim_{dt \to 0} \frac{\mathbb{E}[N(t + dt) - N(t) | \mathcal{H}_t]}{dt}\] <p>which means, in simple words, that the conditional intensity is the expected number of events in a TPP per unit of time.</p> <h2 id="summary">Summary</h2> <p>We have uncovered the mystery of the name “temporal point process”:</p> <ul> <li> <strong>Process</strong> — a TPP can be defined as a counting <em>process</em> </li> <li> <strong>Point</strong> — we can view each TPP realization $\boldsymbol{t} = (t_1, \dots, t_N)$ as a set of <em>“points”</em> </li> <li> <strong>Temporal</strong> — we can interpret the “points” $t_i$ as arrival <em>times</em> of events</li> </ul> <p>We learned about different ways to specify a TPP, such as using the conditional intensity \(\lambda^*(t)\) or the conditional PDFs \(\{f_1^*, f_2^*, f_3^*, \dots\}\).</p> <p>In the next post of this series, I will talk about how we can put this theory to practice and implement neural-network-based TPP models.</p> <h3 id="acknowledgments">Acknowledgments</h3> <p>I would like to thank <a href="https://twitter.com/klicperajo" rel="external nofollow noopener" target="_blank">Johannes Klicpera</a> for his feedback on this post.</p> <h3 id="further-reading">Further reading</h3> <ul> <li> <a href="http://learning.mpi-sws.org/tpp-icml18/" rel="external nofollow noopener" target="_blank">ICML 2018 tutorial by Manuel Gomez Rodriguez and Isabel Valera</a> </li> <li> <a href="https://arxiv.org/abs/1806.00221" rel="external nofollow noopener" target="_blank">Lecture notes by Jakob Rasmussen</a> </li> <li> <a href="https://arxiv.org/abs/1708.06401" rel="external nofollow noopener" target="_blank">A tutorial by Marian-Andrei Rizoiu et al.</a> </li> <li> <a href="https://hawkeslib.readthedocs.io/en/latest/tutorial.html" rel="external nofollow noopener" target="_blank">A tutorial on Hawkes processes by Caner Turkmen</a> </li> </ul> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/"></d-bibliography> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> © Copyright 2024 Oleksandr Shchur. </div> </footer> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"about",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-blog",title:"blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-publications",title:"publications",description:"publications &amp; preprints in reversed chronological order  * denotes equal contribution",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"nav-cv",title:"cv",description:"",section:"Navigation",handler:()=>{window.location.href="/cv/"}},{id:"dropdown-publications",title:"publications",description:"",section:"Dropdown",handler:()=>{window.location.href=""}},{id:"dropdown-blog",title:"blog",description:"",section:"Dropdown",handler:()=>{window.location.href="/blog/"}},{id:"post-temporal-point-processes-2-neural-tpp-models",title:"Temporal Point Processes 2: Neural TPP Models",description:"How can we define flexible TPP models using neural networks?",section:"Posts",handler:()=>{window.location.href="/blog/2021/tpp2-neural-tpps/"}},{id:"post-temporal-point-processes-1-the-conditional-intensity-function",title:"Temporal Point Processes 1: The Conditional Intensity Function",description:"How can we define generative models for variable-length event sequences in continuous time?",section:"Posts",handler:()=>{window.location.href="/blog/2020/tpp1-conditional-intensity/"}},{id:"news-our-work-https-arxiv-org-abs-2104-03528-on-fast-amp-flexible-tpps-has-been-accepted-to-neurips-2020-as-an-oral-presentation",title:"Our [work](https://arxiv.org/abs/2104.03528) on fast &amp; flexible TPPs has been accepted to NeurIPS 2020...",description:"",section:"News"},{id:"news-our-survey-paper-https-arxiv-org-abs-2104-03528-on-neural-tpps-has-been-accepted-to-ijcai-2021-video-summary-https-www-youtube-com-watch-v-j7qh7i0eyfu",title:"Our [survey paper](https://arxiv.org/abs/2104.03528) on neural TPPs has been accepted to IJCAI 2021 [(video...",description:"",section:"News"},{id:"news-i-have-released-a-new-blog-post-blog-2021-tpp2-neural-tpps-about-neural-tpps",title:"I have released a new [blog post](/blog/2021/tpp2-neural-tpps/) about neural TPPs.",description:"",section:"News"},{id:"news-i-39-m-starting-an-internship-at-facebook-ai-research-https-ai-facebook-com-where-i-will-work-with-maximilian-nickel",title:"I&#39;m starting an internship at [Facebook AI Research](https://ai.facebook.com/), where I will work with...",description:"",section:"News"},{id:"news-our-paper-https-arxiv-org-abs-2106-04465-on-anomaly-detection-with-tpps-has-been-accepted-to-neurips-2021",title:"Our [paper](https://arxiv.org/abs/2106.04465) on anomaly detection with TPPs has been accepted to NeurIPS 2021....",description:"",section:"News"},{id:"news-i-will-be-giving-a-talk-on-tpps-at-the-berlin-timeseries-analysis-meetup-slides-assets-pdf-2021-10-12-btsa-presentation-pdf",title:"I will be giving a talk on TPPs at the Berlin Timeseries Analysis...",description:"",section:"News"},{id:"news-i-released-a-library-https-github-com-shchur-survival-distributions-that-extends-the-functionality-of-pytorch-distributions-with-focus-on-tpps",title:"I released [a library](https://github.com/shchur/survival_distributions) that extends the functionality of PyTorch distributions with focus...",description:"",section:"News"},{id:"news-i-joined-aws-ai-in-berlin-where-i-will-work-on-time-series-forecasting-and-the-autogluon-https-auto-gluon-ai-library",title:"I joined AWS AI in Berlin, where I will work on time series...",description:"",section:"News"},{id:"news-check-out-my-phd-thesis-assets-pdf-phd-thesis-pdf-on-neural-temporal-point-processes",title:"Check out my [PhD thesis](/assets/pdf/phd-thesis.pdf) on neural temporal point processes",description:"",section:"News"},{id:"projects-project-1",title:"project 1",description:"with background image",section:"Projects",handler:()=>{window.location.href="/projects/1_project/"}},{id:"projects-project-2",title:"project 2",description:"a project with a background image and giscus comments",section:"Projects",handler:()=>{window.location.href="/projects/2_project/"}},{id:"projects-project-3-with-very-long-name",title:"project 3 with very long name",description:"a project that redirects to another website",section:"Projects",handler:()=>{window.location.href="/projects/3_project/"}},{id:"projects-project-4",title:"project 4",description:"another without an image",section:"Projects",handler:()=>{window.location.href="/projects/4_project/"}},{id:"projects-project-5",title:"project 5",description:"a project with a background image",section:"Projects",handler:()=>{window.location.href="/projects/5_project/"}},{id:"projects-project-6",title:"project 6",description:"a project with no image",section:"Projects",handler:()=>{window.location.href="/projects/6_project/"}},{id:"projects-project-7",title:"project 7",description:"with background image",section:"Projects",handler:()=>{window.location.href="/projects/7_project/"}},{id:"projects-project-8",title:"project 8",description:"an other project with a background image and giscus comments",section:"Projects",handler:()=>{window.location.href="/projects/8_project/"}},{id:"projects-project-9",title:"project 9",description:"another project with an image \ud83c\udf89",section:"Projects",handler:()=>{window.location.href="/projects/9_project/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%6F%6C%65%6B%73.%73%68%63%68%75%72@%67%6D%61%69%6C.%63%6F%6D","_blank")}},{id:"socials-google-scholar",title:"Google Scholar",section:"Socials",handler:()=>{window.open("https://scholar.google.com/citations?user=np39q6IAAAAJ","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/shchur","_blank")}},{id:"socials-rss",title:"RSS Feed",section:"Socials",handler:()=>{window.open("/feed.xml","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>