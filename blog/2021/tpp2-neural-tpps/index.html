<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Temporal Point Processes 2: Neural TPP Models | Oleksandr Shchur </title> <meta name="author" content="Oleksandr Shchur"> <meta name="description" content="How can we define flexible TPP models using neural networks?"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/favicon.ico?d5e016311f0504b4d93d766a59a50c06"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://shchur.github.io/blog/2021/tpp2-neural-tpps/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Temporal Point Processes 2: Neural TPP Models",
            "description": "How can we define flexible TPP models using neural networks?",
            "published": "June 28, 2021",
            "authors": [
              
              {
                "author": "Oleksandr Shchur",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Oleksandr</span> Shchur </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/publications/">publications</a> <div class="dropdown-divider"></div> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/blog/">blog</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Temporal Point Processes 2: Neural TPP Models</h1> <p>How can we define flexible TPP models using neural networks?</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div><a href="#representing-the-data">Representing the data</a></div> <div><a href="#constructing-a-neural-tpp-model">Constructing a neural TPP model</a></div> <ul> <li><a href="#encoding-the-history-into-a-vector">Encoding the history into a vector</a></li> <li><a href="#choosing-a-parametric-distribution">Choosing a parametric distribution</a></li> <li><a href="#conditional-distribution">Conditional distribution</a></li> </ul> <div><a href="#likelihood-function">Likelihood function</a></div> <div><a href="#putting-everything-together">Putting everything together</a></div> <div><a href="#concluding-remarks">Concluding remarks</a></div> </nav> </d-contents> <p>In this post, we will learn about <a href="https://arxiv.org/abs/2104.03528" rel="external nofollow noopener" target="_blank">neural temporal point processes</a> (TPPs) — flexible generative models for variable-length event sequences in continuous time. More specifically, we will</p> <ul> <li>Learn how to parametrize TPPs using neural networks;</li> <li>Derive the likelihood function for TPPs (that we will use as the training objective for our model);</li> <li>Implement a neural TPP model in PyTorch.</li> </ul> <p>If you haven’t read the <a href="https://shchur.github.io/2020/12/17/tpp1-conditional-intensity.html">previous post in the series</a>, I recommend checking it out to get familiar with the main concepts and notation. Alternatively, click on the arrow below to see a short recap of the basics.</p> <details> <summary>Recap</summary> A temporal point process (TPP) is a probability distribution over variable-length event sequnces in a time interval $[0, T]$. We can represent a realization of a TPP as a sequence of <i>strcitly increasing</i> arrival times $\boldsymbol{t} = (t_1, ..., t_N)$, where $N$, the number of events, is itself a random variable. We can specify a TPP by defining $P_i^*(t_i)$, the conditional distribution of the next arrival time $t_i$ given past events $\{t_1, \dots, t_{i-1}\}$, for $i = 1, 2, 3, \dots$. The distribution $P_i^*(t_i)$ can be specified with either a <a target="_blank" href="https://en.wikipedia.org/wiki/Probability_density_function" rel="external nofollow noopener">probability density function</a> (PDF) $f_i^*$, a <a target="_blank" href="https://en.wikipedia.org/wiki/Survival_function" rel="external nofollow noopener">survival function</a> (SF) $S_i^*$, or a <a target="_blank" href="https://en.wikipedia.org/wiki/Failure_rate" rel="external nofollow noopener">hazard function</a> (HF) $h_i^*$. </details> <h2 id="representing-the-data">Representing the data</h2> <p>We will define our neural TPP as an autoregressive model. To do this, at each step $i = 1, 2, 3, …$ we need to specify the distribution \(P_i^*(t_i) := P_i(t_i \vert \mathcal{H}_{t_i})\) of the next arrival time \(t_i\) given the history of past events \(\mathcal{H}_{t_i} = \{t_1, \dots, t_{i-1}\}\). An equivalent but more convenient approach is to instead work with the <em>inter-event</em> times $(\tau_1, \dots, \tau_{N+1})$,<d-footnote>Note that we represent an event sequence $(t_1, \dots, t_N)$ with $N$ events using $N+1$ inter-event times. The last inter-event time $\tau_{N+1}$ corresponds to the time from the last event until $T$, the end of the observed time interval.</d-footnote> where we compute $\tau_i = t_i - t_{i-1}$ (assuming $t_0 = 0$ and $t_{N+1} = T$).</p> <div class="l-body"> <img class="img-fluid rounded" src="/assets/img/posts/tpp2/inter_times.png" style="display: block; width: 80%; margin-left: auto; margin-right: auto;"> <figcaption style="text-align: center; margin-top: 10px; margin-bottom: 10px;"> An event sequence can equivalently be represented by the arrival times $(t_1, \dots, t_N)$ or the inter-event times $(\tau_1, \dots, \tau_{N+1})$. </figcaption> </div> <p>First, let’s load some data and covnert it into a format that can be processed by our model. You can find the dataset and a <a href="https://colab.research.google.com/github/shchur/shchur.github.io/blob/gh-pages/assets/notebooks/tpp2/neural_tpp.ipynb" rel="external nofollow noopener" target="_blank">Jupyter notebook</a> with all the code used in this blog post <a href="https://github.com/shchur/shchur.github.io/blob/gh-pages/assets/notebooks/tpp2/" rel="external nofollow noopener" target="_blank">here</a>.</p> <d-code language="python"> import numpy as np import torch from torch.nn.utils.rnn import pad_sequence data = torch.load("toy_dataset.pkl") # arrival_times_list is a list of variable-length lists # arrival_times_list[j] is the list of arrival times of sequence j arrival_times_list = data["arrival_times"] # t_end = length of the observerd time interval [0, t_end] t_end = data["t_end"] seq_lengths = torch.tensor( [len(t) for t in arrival_times_list], dtype=torch.long ) # (B,) def get_inter_times(t, t_end): tau = np.diff(t, prepend=0.0, append=t_end) return torch.tensor(tau, dtype=torch.float32) inter_times_list = [get_inter_times(t, t_end) for t in arrival_times_list] inter_times = pad_sequence(inter_times_list, batch_first=True) # (B, L) </d-code> <div class="l-body"> <img class="img-fluid rounded" src="/assets/img/posts/tpp2/preprocess_times.png" style="display: block; width: 90%; margin-left: auto; margin-right: auto;"> <figcaption style="text-align: center; margin-top: 10px; margin-bottom: 10px;"> We convert 3 variable-length event sequences of arrival times $\boldsymbol{t}^{(1)}$, $\boldsymbol{t}^{(2)}$, $\boldsymbol{t}^{(3)}$ into a tensor of padded inter-event times of shape <tt>(B, L)</tt>, where <tt>B</tt> - batch size, <tt>L</tt> - padded length. </figcaption> </div> <h2 id="constructing-a-neural-tpp-model">Constructing a neural TPP model</h2> <p>How can we parametrize the conditional distribution \(P_i^*(\tau_i)\) with a neural network? A simple and elegant answer to this question was proposed in the seminal work by Du, Dai, Trivedi, Gomez-Rodriguez and Song <d-cite key="du2016recurrent"></d-cite>:</p> <ol> <li>Encode the event history \(\mathcal{H}_{t_i} = \{t_1, \dots, t_{i-1}\}\) into a <em>fixed-dimensional</em> context vector \(\boldsymbol{c}_i \in \mathbb{R}^C\) using a neural network.</li> <li>Pick a parametric probability density function \(f(\cdot \vert \boldsymbol{\theta})\) that defines the distribution of a positive<d-footnote>We always assume that the arrival times are sorted, that is, $t_i &lt; t_{i+1}$ for all $i$. Therefore, the inter-event times $\tau_i$ are strictly positive.</d-footnote> random variable (e.g., PDF of the <a href="https://en.wikipedia.org/wiki/Exponential_distribution" rel="external nofollow noopener" target="_blank">exponential distribution</a> or <a href="https://en.wikipedia.org/wiki/Weibull_distribution" rel="external nofollow noopener" target="_blank">Weibull distribution</a>).</li> <li>Use the context vector \(\boldsymbol{c}_i\) to obtain the parameters \(\boldsymbol{\theta}_i\). Plug in \(\boldsymbol{\theta}_i\) into \(f(\cdot \vert \boldsymbol{\theta})\) to obtain the PDF \(f(\tau_i \vert \boldsymbol{\theta}_i)\) of the conditional distribution \(P_i^*(\tau_i)\).</li> </ol> <p>We will now look at each of these steps in more detail.</p> <div class="l-body"> <img class="img-fluid rounded" src="/assets/img/posts/tpp2/architecture.png" style="display: block; width: 90%; margin-left: auto; margin-right: auto;"> <figcaption style="text-align: center; margin-top: 10px; margin-bottom: 10px;"> Schematic representation of the neural TPP model that we will implement today. </figcaption> </div> <h3 id="encoding-the-history-into-a-vector">Encoding the history into a vector</h3> <p>The original approach by Du et al. <d-cite key="du2016recurrent"></d-cite> uses a recurrent neural network (RNN) to encode the event history into a vector. This works as follows.</p> <ol type="i"> <li>Each event $t_j$ is represented by a feature vector $\boldsymbol{y}_j$. In our model, we will simply define $\boldsymbol{y}_j = (\tau_j, \log \tau_j)^T$.<d-footnote>Using the logarithm here allows the model to distinguish between very small inter-event times.</d-footnote> More sophisticated approaches, like positional encoding with trigonometric functions <d-cite key="zhang2020self"></d-cite>, are also possible.</li> <li>We initialize $\boldsymbol{c}_1$ (for example, to a vector of all zeros). The vector $\boldsymbol{c}_1$ will work both as the initial state of the RNN, as well as to obtain the parameters of $P_1^*(\tau_1)$.</li> <li>After each event $t_i$, we compute the next the context vector $\boldsymbol{c}_{i+1}$ (i.e., the next hidden state of the RNN) based on the previous state $\boldsymbol{c}_i$ and features $\boldsymbol{y}_i$ $$ \boldsymbol{c}_{i+1} = \operatorname{Update}(\boldsymbol{c}_i, \boldsymbol{y}_{i}). $$ The specific RNN architecture is not very important here — <a target="_blank" href="https://pytorch.org/docs/stable/generated/torch.nn.RNN.html" rel="external nofollow noopener">vanilla RNN</a>, <a target="_blank" href="https://pytorch.org/docs/stable/generated/torch.nn.GRU.html" rel="external nofollow noopener">GRU</a> or <a target="_blank" href="https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html" rel="external nofollow noopener">LSTM</a> update functions can all be used here. By processing the entire sequence $(t_1, \dots, t_N)$, we compute all the context vectors $(\boldsymbol{c}_1, \dots, \boldsymbol{c}_{N+1})$. </li> </ol> <d-code language="python"> import torch.nn as nn import torch.nn.functional as F context_size = 32 rnn = nn.GRU(input_size=2, hidden_size=context_size, batch_first=True) def get_context(inter_times): # inter_times: Padded inter-event times, shape (B, L) tau = inter_times.unsqueeze(-1) # (B, L, 1) # Clamp tau to avoid computing log(0) for padding and getting NaNs log_tau = inter_times.clamp_min(1e-8).log().unsqueeze(-1) # (B, L, 1) rnn_input = torch.cat([tau, log_tau], dim=-1) # (B, L, 2) # The intial state is automatically set to zeros rnn_output = rnn(rnn_input) # (B, L, C) # Shift by one such that context[:, i] will be used # to parametrize the distribution of inter_times[:, i] context = F.pad(rnn_output[:, :-1, :], (0, 0, 1, 0)) # (B, L, C) return context </d-code> <div class="l-body"> <img class="img-fluid rounded" src="/assets/img/posts/tpp2/padding.png" style="display: block; width: 90%; margin-left: auto; margin-right: auto;"> <figcaption style="text-align: center; margin-top: 10px; margin-bottom: 10px;"> We shift the <tt>rnn_output</tt> by one and add padding to obtain the <tt>context</tt> tensor that is properly aligned with the <tt>inter_times</tt>. </figcaption> </div> <h3 id="choosing-a-parametric-distribution">Choosing a parametric distribution</h3> <p>When picking a parametric distribution for inter-event times, we have to make sure that its probability density function (PDF) $f(\cdot \vert \boldsymbol{\theta})$ and survival function $S(\cdot \vert \boldsymbol{\theta})$ can be computed analytically — we will need this later when computing the log-likelihood. For some applications, it’s also nice to able to sample from the distribution analytically. I decided to use <a target="_blank" href="https://en.wikipedia.org/wiki/Weibull_distribution#Alternative_parameterizations" rel="external nofollow noopener">Weibull distribution</a> here as it satisfies all these properties.<d-footnote>Many other distributions over $[0, \infty)$ also satisfy these properties (e.g., exponential, log-normal, log-logistic, Gompertz distributions, or their mixtures), but some don't. For example, computing the survival function and sampling are not straightforward for the gamma distribution.</d-footnote></p> <div class="l-body"> <img class="img-fluid rounded" src="/assets/img/posts/tpp2/weibull.png" style="display: block; width: 100%; margin-left: auto; margin-right: auto;"> <figcaption style="text-align: center; margin-top: 10px; margin-bottom: 10px;"> PDF of the Weibull distribution with different values of the parameters $b$ and $k$. </figcaption> </div> <p>The Weibull distribution has two strictly positive parameters $\boldsymbol{\theta} = (b, k)$. The probability density function is computed as \[ f(\tau \vert b, k) = b k \tau^{k-1} \exp(-b \tau^{k}) \] and the survival function is \[ S(\tau \vert b, k) = \exp(-b \tau^{k}). \]</p> <p>We implement the Weibull distribution using an API that is similar to <tt><a href="https://pytorch.org/docs/stable/distributions.html" rel="external nofollow noopener" target="_blank">torch.distributions</a></tt>.</p> <d-code language="python"> class Weibull: def __init__(self, b, k, eps=1e-8): # b and k are strictly positive tensors of the same shape self.b = b self.k = k self.eps = eps def log_prob(self, x): """Logarithm of the probability density function log(f(x)).""" # x must have the same shape as self.b and self.k x = x.clamp_min(self.eps) # pow is unstable for inputs close to 0 return (self.b.log() + self.k.log() + (self.k - 1) * x.log() + self.b.neg() * torch.pow(x, self.k)) def log_survival(self, x): """Logarithm of the survival function log(S(x)).""" x = x.clamp_min(self.eps) return self.b.neg() * torch.pow(x, self.k) </d-code> <h3 id="conditional-distribution">Conditional distribution</h3> <p>To obtain the conditional PDF $f^*_i(\tau_i) := f(\tau_i | k_i, b_i)$ of the next inter-event time given the history, we compute the parameters $k_i$, $b_i$ using the most recent context embedding $\boldsymbol{c}_i$ \[ \begin{aligned} k_i = \sigma(\boldsymbol{v}^T_k \boldsymbol{c}_i + d_k) &amp; \qquad &amp; b_i = \sigma(\boldsymbol{v}^T_b \boldsymbol{c}_i + d_b). \end{aligned} \] Here, $\boldsymbol{v}_k \in \mathbb{R}^C, \boldsymbol{v}_b \in \mathbb{R}^C, d_k \in \mathbb{R}, d_b \in \mathbb{R}$ are learnable weights, and $\sigma: \mathbb{R} \to (0, \infty)$ is a nonlinear function that ensures that the parameters are strictly positive (e.g., softplus).</p> <d-code language="python"> hypernet = nn.Linear(in_features=context_size, out_features=2) def get_inter_time_distribution(context): # context has shape (B, L, C) raw_params = hypernet(context) # (B, L, 2) b = F.softplus(raw_params[..., 0]) # (B, L) k = F.softplus(raw_params[..., 1]) # (B, L) return Weibull(b, k) </d-code> <p>The weights $\{\boldsymbol{v}_k, \boldsymbol{v}_b, d_k, d_b\}$ together with the weights of the RNN are the learnable parameters of our neural TPP model. The next question we have to answer is how to estimate these from data.</p> <h2 id="likelihood-function">Likelihood function</h2> <p>Log-likelihood (LL) is the default training objective for generative probabilistic models, and TPPs are no exception. To derive the likelihood function for a TPP we will start with a simple example.</p> <p>Suppose we have observed a single event with arrival time \(t_1\) in the time interval \([0, T]\). We can describe this outcome as “the first event happened in $[t_1, t_1 + dt)$ and the second event happened after $T$”. <d-footnote>Here $dt$ denotes an infinitesimal positive number.</d-footnote> The probability of this outcome is</p> \[\begin{align} \begin{split} p(\{t_1\}) =&amp; \Pr(\text{1st event in $[t_1, t_1 + dt)$})\\ &amp; \times \Pr(\text{2nd event after $T$} \mid t_1)\\ =&amp; f_1^*(t_1) dt \times S_2^*(T) \end{split} \end{align}\] <p>The equality here follows simply from the definition of the PDF \(f_1^*\) and the survival function \(S_2^*\) (as discussed in the <a href="https://shchur.github.io/blog/2020/tpp1-conditional-intensity/">previous post</a>).<d-footnote>We can make another interesting observation here. When computing the probability in Equation (1), we could also consider the event $$ \{\text{3rd event after $T$} \mid \text{1st event at $t_1$, 2nd event after $T$} \}. $$ However, the probability of this event is equal to 1, since we already know that the second event happened after $T$ and, by definition, the third event happens after the second event. The same holds for events like $\{\text{4th event after $T$} \mid ... \}$. Therefore, even though a TPP realization may contain an arbitrary large number of events, when computing the the likelihood of a particular sequence $\boldsymbol{t} = (t_1, \dots, t_N)$, we only need to consider $N+1$ terms.</d-footnote> Following the same reasoning, we compute the likelihood for an arbitrary sequence \(\boldsymbol{t} = (t_1, t_2, \dots, t_N)\) consisting of \(N\) events as</p> \[\begin{align} p(\boldsymbol{t}) = (dt)^N \left(\prod_{i=1}^{N} f_i^*(t_i)\right) S_{N+1}^*(T). \end{align}\] <p>The $(dt)^N$ term is just a multiplicative constant with respect to the model parameters, so we ignore it during optimization. By taking the logarithm, we get the log-likelihood</p> \[\begin{align} \log p(\boldsymbol{t}) &amp;= \left(\sum_{i=1}^{N} \log f_i^*(t_i)\right) + \log S_{N+1}^*(T). \end{align}\] <p>Lastly, by slightly abusing the notation, we switch back to the inter-event times and obtain</p> \[\begin{align} \log p(\boldsymbol{t}) &amp;= \left(\sum_{i=1}^{N} \log f_i^*(\tau_i)\right) + \log S_{N+1}^*(\tau_{N+1}). \end{align}\] <p>We will use this formulation of the log-likelihood to train our neural TPP model.</p> <p>It’s worth noting that Equation (4) is not the only way to express the log-likelihood of a TPP. In the previous post, we talked about different functions characterizing a TPP, such as conditional hazard functions \(\{h_1^*, h_2^*, h_3^*...\}\) and the conditional intensity function \(\lambda^*(t)\). Many papers and textbooks work with these functions instead. Click on the arrow below for more details.</p> <details> <summary>Other ways to compute the log-likelihood of a TPP</summary> We combine Equation (3) with the definition of the conditional hazard function $h_i^*(t) = f_i^*(t)/S_i^*(t)$ and obtain $$ \begin{aligned} \log p(\boldsymbol{t}) =&amp; \left(\sum_{i=1}^{N} \log f_i^*(t_i)\right) + \log S_{N+1}^*(T)\\ =&amp; \left(\sum_{i=1}^{N} \log h_i^*(t_i) + \log S_i^*(t_i)\right) + \log S_{N+1}^*(T)\\ =&amp; \sum_{i=1}^{N} \log h_i^*(t_i) + \sum_{i=1}^{N+1} \log S_i^*(t_i), \end{aligned} $$ where we defined $t_{N+1}=T$. Last time, we derived the equality <d-footnote> From the definition of the hazard function, it follows $$ \begin{aligned} h_i^*(t) &amp;= \frac{f_i^*(t)}{S_i^*(t)} = \frac{- \frac{d}{dt} S_i^*(t)}{S_i^*(t)} = -\frac{d}{dt} \log S_i^*(t)\\ &amp; \Leftrightarrow S_i^*(t) = \exp \left( -\int_{t_{i-1}}^t h_i^*(u) du \right) \end{aligned} $$</d-footnote> $$S_i^*(t) = \exp \left(-\int_{t_{i-1}}^{t} h_i^*(u) du\right).$$ Plugging this into the expression for the log-likelihood, we get $$ \begin{aligned} \log p(\boldsymbol{t}) =&amp; \sum_{i=1}^{N} \log h_i^*(t_i) - \sum_{i=1}^{N+1} \left(\int_{t_{i-1}}^{t_i} h_i^*(u) du\right) \end{aligned} $$ Finally, using the definition of the conditional intensity,<d-footnote> The conditional intensity is defined piecewise by stitching together the hazard functions $$ \lambda^*(t) = \begin{cases} h_1^*(t) &amp; \text{ if } 0 \le t \le t_1 \\ h_2^*(t) &amp; \text{ if } t_1 &lt; t \le t_2 \\ &amp; \vdots\\ h_{N+1}^*(t) &amp; \text{ if } t_N &lt; t \le T \\ \end{cases} $$ </d-footnote> we rewrite the log-likelihood as $$ \begin{aligned} \log p(\boldsymbol{t}) =&amp; \sum_{i=1}^{N} \log \lambda^*(t_i) - \sum_{i=1}^{N+1} \left(\int_{t_{i-1}}^{t_i} \lambda^*(u) du\right)\\ =&amp; \sum_{i=1}^{N} \log \lambda^*(t_i) - \int_{0}^{T} \lambda^*(u) du. \end{aligned} $$ You will often see this expression for the LL in papers and textbooks. As we have just showed, it is exactly equivalent to both Equations (3) and (4) that we derived before. </details> <p><br></p> <p>The trickiest part when implementing TPP models is vectorizing the operations on variable-length sequences (i.e., avoiding for-loops). This is usually done with masking and operations like <tt>torch.gather</tt>. For example, here is a vectorized implementation of the negative log-likelihood (NLL) for our neural TPP model.</p> <d-code language="python"> def nll_loss(inter_times, seq_lengths): # inter_times: Padded inter-event times, shape (B, L) # seq_lengths: Number of events in each sequence, shape (B,) context = get_context(inter_times) # (B, L, C) inter_time_dist = get_inter_time_distribution(context) log_pdf = inter_time_dist.log_prob(inter_times) # (B, L) # Construct a boolean mask that selects observed events arange = torch.arange(inter_times.shape[1], device=seq_lengths.device) mask = (arange[None, :] &lt; seq_lengths[:, None]).float() # (B, L) log_like = (log_pdf * mask).sum(-1) # (B,) log_surv = inter_time_dist.log_survival(inter_times) # (B, L) end_idx = seq_lengths.unsqueeze(-1) # (B, 1) log_surv_last = torch.gather(log_surv, dim=-1, index=end_idx) # (B, 1) log_like += log_surv_last.squeeze(-1) # (B,) return -log_like </d-code> <div class="l-body"> <img class="img-fluid rounded" src="/assets/img/posts/tpp2/nll_computation.png" style="display: block; width: 90%; margin-left: auto; margin-right: auto;"> <figcaption style="text-align: center; margin-top: 10px; margin-bottom: 10px;"> Log-likelihood of a sequence $(\tau_1, ..., \tau_{N+1})$ is computed as $\left(\sum_{i=1}^{N} \log f^*_i(\tau_i)\right) + \log S_{N+1}^*(\tau_{N+1})$. In the above figure this corresponds to summing up the orange entries in each row. </figcaption> </div> <h2 id="putting-everything-together">Putting everything together</h2> <p>Now we have all the pieces necessary to define and train our model. Here is a link to the <a href="https://colab.research.google.com/github/shchur/shchur.github.io/blob/gh-pages/assets/notebooks/tpp2/neural_tpp.ipynb" rel="external nofollow noopener" target="_blank">Jupyter notebook</a> with all the code we’ve seen so far, but where the different model components are nicely wrapped into a single <tt>nn.Module</tt>.</p> <p>As mentioned before, we train the model by minimizing the NLL of the training sequences. More specifically, we average the loss over all sequences and normalize it by $T$, the length of the observed time interval.</p> <d-code language="python"> model = NeuralTPP() opt = torch.optim.Adam(model.parameters(), lr=5e-3) max_epochs = 200 for epoch in range(max_epochs): opt.zero_grad() loss = model.nll_loss(inter_times, seq_lengths).mean() / t_end loss.backward() opt.step() </d-code> <p>There are different ways to evaluate TPP models. Here, I chose to visualize some properties of the event sequences generated by the model and compare them to those of the training data. The code for sampling is not particularly interesting. It follows the same logic as before — at each step $i$, we sample the next inter-event time $\tau_i \sim f_i^*(\tau_i)$, feed it into the RNN to obtain the next context embedding $\boldsymbol{c}_{i+1}$, and repeat the procedure. See the <a href="https://colab.research.google.com/github/shchur/shchur.github.io/blob/gh-pages/assets/notebooks/tpp2/neural_tpp.ipynb" rel="external nofollow noopener" target="_blank">Jupyter notebook</a> for details.</p> <div class="l-body"> <img class="img-fluid rounded" src="/assets/img/posts/tpp2/visualize_results.png" style="display: block; width: 100%; margin-left: auto; margin-right: auto;"> <figcaption style="text-align: center; margin-top: 10px; margin-bottom: 10px;"> Comparison of real and generated event sequences. <br> <b>Left:</b> Visualization of the arrival times in 10 real (top) and 10 simulated (bottom) sequences. <br> <b>Right:</b> Distribution of sequence lengths for real (top) and simulated (bottom) event sequences. </figcaption> </div> <p>We see that training sequences have a trend: there are a lot more events in the $[0, 50]$ interval than in $[50, 100]$ (top left figure). Our neural TPP model has learned to produce sequences with a similar property (bottom left). The figure also shows that the distribution of the sequence lengths in real (top right) and simulated (bottom right) sequences are quite similar. We conclude that the model has approximated the true data-generating TPP reasonably well. Of course, there is room for improvement. After all, we defined a really simple architecture, and it’s possible to get even better results by using a more flexible model.</p> <h2 id="concluding-remarks">Concluding remarks</h2> <p>In this post, we have learned about the general design principles of neural TPPs and implemented a simple model of this class. Unlike traditional TPP models that we discussed in the previous post (Poisson processes, Hawkes processes), neural TPPs can simultaneously capture different patterns in the data (e.g., global trends, burstiness, repeating subsequences).</p> <p>Neural TPPs are a hot research topic, and a number of improvements have been proposed in the last couple of years. For example, one can use a transformer as the history encoder <d-cite key="zhang2020self,zuo2020transformer"></d-cite>, or choose a more flexible parametrization of the conditional distribution <d-cite key="omi2019fully,shchur2020intensity,zhang2020cause"></d-cite>. Some works take a completely different approach — they directly parametrize the conditional intenisity $\lambda^*(t)$ using a hidden state that evolves in continuous time according to a (neural) ODE <d-cite key="de2019gru,rubanova2019latent"></d-cite>, instead of defining the model autoregressively. If you want to learn more about neural TPPs, their applications, and open challenges, you can check our recent survey paper <d-cite key="shchur2021neural"></d-cite>.</p> <p>So far we have been talking exclusively about the so-called <em>unmarked</em> TPPs, where each event is represented only by its arrival time $t_i$. Next time, I will talk about the important case of <em>marked</em> TPPs, where events come with additional information, such as class labels or spatial locations.</p> <h3 id="acknowledgments">Acknowledgments</h3> <p>I would like to thank <a href="https://twitter.com/DanielZuegner" rel="external nofollow noopener" target="_blank">Daniel Zügner</a> for his feedback on this post.</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/2021-06-28-tpp2.bib"></d-bibliography> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> © Copyright 2024 Oleksandr Shchur. </div> </footer> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"about",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-blog",title:"blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-publications",title:"publications",description:"publications &amp; preprints in reversed chronological order  * denotes equal contribution",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"dropdown-publications",title:"publications",description:"",section:"Dropdown",handler:()=>{window.location.href=""}},{id:"dropdown-blog",title:"blog",description:"",section:"Dropdown",handler:()=>{window.location.href="/blog/"}},{id:"post-temporal-point-processes-2-neural-tpp-models",title:"Temporal Point Processes 2: Neural TPP Models",description:"How can we define flexible TPP models using neural networks?",section:"Posts",handler:()=>{window.location.href="/blog/2021/tpp2-neural-tpps/"}},{id:"post-temporal-point-processes-1-the-conditional-intensity-function",title:"Temporal Point Processes 1: The Conditional Intensity Function",description:"How can we define generative models for variable-length event sequences in continuous time?",section:"Posts",handler:()=>{window.location.href="/blog/2020/tpp1-conditional-intensity/"}},{id:"news-our-work-https-arxiv-org-abs-2104-03528-on-fast-amp-flexible-tpps-has-been-accepted-to-neurips-2020-as-an-oral-presentation",title:"Our [work](https://arxiv.org/abs/2104.03528) on fast &amp; flexible TPPs has been accepted to NeurIPS 2020...",description:"",section:"News"},{id:"news-our-survey-paper-https-arxiv-org-abs-2104-03528-on-neural-tpps-has-been-accepted-to-ijcai-2021-video-summary-https-www-youtube-com-watch-v-j7qh7i0eyfu",title:"Our [survey paper](https://arxiv.org/abs/2104.03528) on neural TPPs has been accepted to IJCAI 2021 [(video...",description:"",section:"News"},{id:"news-i-have-released-a-new-blog-post-blog-2021-tpp2-neural-tpps-about-neural-tpps",title:"I have released a new [blog post](/blog/2021/tpp2-neural-tpps/) about neural TPPs.",description:"",section:"News"},{id:"news-i-39-m-starting-an-internship-at-facebook-ai-research-https-ai-facebook-com-where-i-will-work-with-maximilian-nickel",title:"I&#39;m starting an internship at [Facebook AI Research](https://ai.facebook.com/), where I will work with...",description:"",section:"News"},{id:"news-our-paper-https-arxiv-org-abs-2106-04465-on-anomaly-detection-with-tpps-has-been-accepted-to-neurips-2021",title:"Our [paper](https://arxiv.org/abs/2106.04465) on anomaly detection with TPPs has been accepted to NeurIPS 2021....",description:"",section:"News"},{id:"news-i-will-be-giving-a-talk-on-tpps-at-the-berlin-timeseries-analysis-meetup-slides-assets-pdf-2021-10-12-btsa-presentation-pdf",title:"I will be giving a talk on TPPs at the Berlin Timeseries Analysis...",description:"",section:"News"},{id:"news-i-released-a-library-https-github-com-shchur-survival-distributions-that-extends-the-functionality-of-pytorch-distributions-with-focus-on-tpps",title:"I released [a library](https://github.com/shchur/survival_distributions) that extends the functionality of PyTorch distributions with focus...",description:"",section:"News"},{id:"news-i-joined-aws-ai-in-berlin-where-i-will-work-on-time-series-forecasting-and-the-autogluon-https-auto-gluon-ai-library",title:"I joined AWS AI in Berlin, where I will work on time series...",description:"",section:"News"},{id:"news-check-out-my-phd-thesis-assets-pdf-phd-thesis-pdf-on-neural-temporal-point-processes",title:"Check out my [PhD thesis](/assets/pdf/phd-thesis.pdf) on neural temporal point processes",description:"",section:"News"},{id:"news-we-have-released-chronos-a-family-of-pretrained-models-for-time-series-forecasting-code-https-github-com-amazon-science-chronos-forecasting-model-weights-https-huggingface-co-collections-amazon-chronos-models-and-datasets-65f1791d630a8d57cb718444",title:"We have released Chronos, a family of pretrained models for time series forecasting...",description:"",section:"News"},{id:"news-we-have-released-autogluon-v1-1-https-github-com-autogluon-autogluon-releases-tag-v1-1-0-now-including-pretrained-models-for-time-series-forecasting",title:"We have released [AutoGluon v1.1](https://github.com/autogluon/autogluon/releases/tag/v1.1.0), now including pretrained models for time series forecasting!...",description:"",section:"News"},{id:"news-i-have-presented-our-latest-work-on-time-series-forecasting-at-the-automl-summer-school-in-hannover-amp-automl-conference-in-paris-slides-assets-pdf-2024-09-04-automl-summer-school-pdf",title:"I have presented our latest work on time series forecasting at the AutoML...",description:"",section:"News"},{id:"projects-project-1",title:"project 1",description:"with background image",section:"Projects",handler:()=>{window.location.href="/projects/1_project/"}},{id:"projects-project-2",title:"project 2",description:"a project with a background image and giscus comments",section:"Projects",handler:()=>{window.location.href="/projects/2_project/"}},{id:"projects-project-3-with-very-long-name",title:"project 3 with very long name",description:"a project that redirects to another website",section:"Projects",handler:()=>{window.location.href="/projects/3_project/"}},{id:"projects-project-4",title:"project 4",description:"another without an image",section:"Projects",handler:()=>{window.location.href="/projects/4_project/"}},{id:"projects-project-5",title:"project 5",description:"a project with a background image",section:"Projects",handler:()=>{window.location.href="/projects/5_project/"}},{id:"projects-project-6",title:"project 6",description:"a project with no image",section:"Projects",handler:()=>{window.location.href="/projects/6_project/"}},{id:"projects-project-7",title:"project 7",description:"with background image",section:"Projects",handler:()=>{window.location.href="/projects/7_project/"}},{id:"projects-project-8",title:"project 8",description:"an other project with a background image and giscus comments",section:"Projects",handler:()=>{window.location.href="/projects/8_project/"}},{id:"projects-project-9",title:"project 9",description:"another project with an image \ud83c\udf89",section:"Projects",handler:()=>{window.location.href="/projects/9_project/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%6F%6C%65%6B%73.%73%68%63%68%75%72@%67%6D%61%69%6C.%63%6F%6D","_blank")}},{id:"socials-google-scholar",title:"Google Scholar",section:"Socials",handler:()=>{window.open("https://scholar.google.com/citations?user=np39q6IAAAAJ","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/shchur","_blank")}},{id:"socials-rss",title:"RSS Feed",section:"Socials",handler:()=>{window.open("/feed.xml","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>